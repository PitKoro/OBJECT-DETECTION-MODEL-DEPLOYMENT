version: '3.9'
services:

  triton:
    image: nvcr.io/nvidia/tritonserver:22.07-py3
    shm_size: '2gb'
    container_name: triton
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    volumes:
      - ./triton_server/models:/models    
    command: 'tritonserver --model-repository=/models'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: "curl -v localhost:8000/v2/health/ready"

  jupyter:
    build: ./detectron-train-export-to-torchscript
    depends_on:
      triton:
        condition: service_healthy
    ports:
    - 8888:8888
    volumes:
      - ./detectron-train-export-to-torchscript/dataset/:/app/dataset
      - ./detectron-train-export-to-torchscript/checkpoints/:/app/checkpoints
      - ./detectron-train-export-to-torchscript/src/:/app/src
      - ./detectron-train-export-to-torchscript/configs/:/app/configs
      - ./detectron-train-export-to-torchscript/notebooks/:/app/notebooks
    container_name: jupyter
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]