version: '3.9'
services:

  triton:
    image: nvcr.io/nvidia/tritonserver:22.07-py3
    shm_size: '2gb'
    container_name: triton
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    volumes:
      - ./triton/models:/models    
    command: 'tritonserver --model-repository=/models'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # healthcheck:
      # test: "curl -v localhost:8000/v2/health/ready"

  kserve:
    build: ./kserve
    ports:
      - 8080:8080
    volumes:
      - ./kserve/configs/:/app/configs
      - ./kserve/weights/:/app/weights
    container_name: kserve
    command: 'python app.py'
    # healthcheck:
    #   test: "curl -v localhost:8080/v2/health/ready"

  jupyter:
    build: ./jupyter
    # depends_on:
    #   kserve:
    #     condition: service_healthy
    #   triton:
    #     condition: service_healthy
    ports:
    - 8888:8888
    - 5151:5151
    volumes:
      - ./jupyter/dataset/:/app/dataset
      - ./jupyter/checkpoints/:/app/checkpoints
      - ./jupyter/src/:/app/src
      - ./jupyter/configs/:/app/configs
      - ./jupyter/notebooks/:/app/notebooks
    container_name: jupyter
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]